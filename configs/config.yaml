project_name: "RL-Portfolio-Management_IMPROVEMENTS"
epochs: 25
time_window: 30
rl_strats: ["PPOLSTM"]
baseline: ["RANDOM"]
varied_base_seeds: [1, 3, 5, 7, 9]
normalise_data: false
wandb_state: "disabled"
noises:
  [0, 0.001, 0.002, 0.004, 0.005, 0.006, 0.008, 0.01, 0.02, 0.05, 0.1, 0.2]

env:
  redownload: True
  stocks: "ALL"
  start_date: "2009-01-01"
  period: "1D"
  base_seed: 9
  transaction_cost: 0.0002
  start_cash: 10000
  perturbation_noise: 0

agent:
  gamma: 0.99
  learning_rate: 0.0003
  reward_function: "Standard Logarithmic Returns"
  ppo:
    gae_lambda: 0.98
    clip_param: 0.2
    batch_size: 24
    fc1_n: 128
    fc2_n: 128
    feature_output_size: 128
    epochs: 1
    entropy_coef: 0.01
    actor_noise: 0
    norm_advantages: false
    use_entropy: false
    use_dirichlet: true
    log_concentration: false
    use_normals: false # likely requires softmax; dividing may not work for short selling
    learning_frequency: 24
    actor_critic_hidden_state_size: 512
    strategy: "PPOLSTM"

hyperparameters:
  feature_output_sizes: [32, 64, 128, 256, 512]
  learning_rates: [0.0001, 0.0003, 0.0005, 0.0007]

evaluation:
  log_observations: false
  log_input_data: false

#argparse or something?
experiments:
  _common: &defaults
    dataType: "validation"
    benchmark: false
    comparisonStrat: null
    use_noise_eval: false
    for_learning_curve: false

  data_normalisation:
    <<: *defaults

  noise_testing:
    <<: *defaults

  hyperparameter_tuning:
    <<: *defaults

  reward_testing:
    dataType: "testing"
    benchmark: null
    comparisonStrat: null
    use_noise_eval: false #possible comparison with perturbed?
    for_learning_curve: True

  random:
    <<: *defaults
    benchmark: true
    use_noise_eval: true

  nonRLComparisonStrategies:
    <<: *defaults
    dataType: "testing"

test:
  learning_curve_frequency: 500
